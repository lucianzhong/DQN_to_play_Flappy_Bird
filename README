
Installation Dependencies:
Python 2.7 or 3
TensorFlow 0.7
pygame
OpenCV-Python

How to run?
run DQN_angent.py


What is Deep Q-Network?
It is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards


The pseudo-code for the DQN:

Initialize replay memory D to size N
Initialize action-value function Q with random weights
for episode = 1, M do
    Initialize state s_1
    for t = 1, T do
        With probability ϵ select random action a_t
        otherwise select a_t=max_a  Q(s_t,a; θ_i)
        Execute action a_t in emulator and observe r_t and s_(t+1)
        Store transition (s_t,a_t,r_t,s_(t+1)) in D
        Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D
        Set y_j:=
            r_j for terminal s_(j+1)
            r_j+γ*max_(a^' )  Q(s_(j+1),a'; θ_i) for non-terminal s_(j+1)
        Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^2 with respect to θ
    end for
end for



DQN这篇文章的算法就是Q-learning+function approximation（只不过function approximation是比较特殊的），每一次根据所获得的来更新Q-value，
本质就是stochastic gradient descent (SGD)。一般在用mini-batch SGD做优化时，都假设样本之间的相对独立，从而使得每个mini-batch内数据所含的噪声相互抵消，
算法收敛的更快。在这个问题中，之所以加入experience replay是因为样本是从游戏中的连续帧获得的，这与简单的reinforcement learning问题（比如maze）相比，
样本的关联性大了很多，如果没有experience replay，算法在连续一段时间内基本朝着同一个方向做gradient descent，那么同样的步长下这样直接计算gradient就有可能不收敛。
因此experience replay是从一个memory pool中随机选取了一些expeirence，然后再求梯度，从而避免了这个问题。
原文的实验中指出mini batch是32，而replay memory存了最近的1000000帧，可以看出解决关联性的问题在DQN里是个比较重要的技巧。

Reference:
https://github.com/yenchenlin/DeepLearningFlappyBird
